# agente_pesquisa_contratos.py - Vers√£o 2.0 (Pesquisa Persistente e Aprofundada)

import asyncio
import aiohttp
import re
from datetime import datetime
from typing import Dict, Any, List
from googlesearch import search
from bs4 import BeautifulSoup

class AgentePesquisaContratos:
    """
    Agente de Pesquisa Otimizado e Especializado em encontrar modelos e cl√°usulas de contratos.
    v2.0: Realiza uma pesquisa persistente, garantindo um n√∫mero m√≠nimo de extra√ß√µes bem-sucedidas.
    """
    def __init__(self):
        print("üîç Inicializando Agente de Pesquisa de CONTRATOS (Persistente v2.0)...")
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        # COMENT√ÅRIO: Novas configura√ß√µes para a pesquisa persistente.
        self.config = {
            'tamanho_minimo_conteudo': 500,
            'tamanho_maximo_conteudo': 20000,
            'min_sucessos_por_termo': 4, # META: Garantir pelo menos 4 conte√∫dos por termo.
            'google_search_results': 10, # Busca mais links para ter mais op√ß√µes.
        }
        self.sites_prioritarios = ['jusbrasil.com.br', 'conjur.com.br', 'migalhas.com.br', 'planalto.gov.br']
        print("‚úÖ Sistema de pesquisa de CONTRATOS inicializado.")

    async def _extrair_conteudo_url_async(self, session, url: str) -> Dict[str, Any]:
        """Extrai conte√∫do de uma URL de forma ass√≠ncrona com logs detalhados."""
        print(f"‚Üí Tentando extrair de: {url}")
        try:
            async with session.get(url, headers=self.headers, timeout=15, ssl=False) as response:
                if response.status == 200:
                    raw_html = await response.read()
                    html = raw_html.decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html, 'html.parser')
                    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header']):
                        tag.decompose()
                    texto = soup.body.get_text(separator='\n', strip=True) if soup.body else ""
                    texto_limpo = re.sub(r'\n\s*\n', '\n', texto).strip()
                    
                    if len(texto_limpo) < self.config['tamanho_minimo_conteudo']:
                        print(f"‚ö†Ô∏è Descartado (curto): {url}")
                        return None

                    print(f"‚úî SUCESSO: Conte√∫do extra√≠do de {url} ({len(texto_limpo)} caracteres)")
                    return {"url": url, "texto": texto_limpo[:self.config['tamanho_maximo_conteudo']]}
                else:
                    print(f"‚ùå Falha (Status {response.status}): {url}")
                    return None
        except Exception as e:
            print(f"‚ùå Falha (Erro: {type(e).__name__}): {url}")
            return None

    async def _pesquisar_e_extrair_async(self, termo: str) -> List[Dict[str, Any]]:
        """
        COMENT√ÅRIO: L√≥gica principal aprimorada. Agora ele busca mais links e tenta extrair
        at√© atingir a meta de sucessos, ignorando as falhas.
        """
        print(f"\nüìö Buscando modelos e cl√°usulas para: '{termo}'...")
        site_query = " OR ".join([f"site:{site}" for site in self.sites_prioritarios])
        query = f'"{termo}" {site_query}'
        
        resultados_sucesso = []
        urls_tentadas = set()
        
        try:
            loop = asyncio.get_event_loop()
            urls_google = await loop.run_in_executor(None, lambda: list(search(query, num_results=self.config['google_search_results'], lang="pt")))
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                for url in urls_google:
                    if url not in urls_tentadas:
                        urls_tentadas.add(url)
                        tasks.append(self._extrair_conteudo_url_async(session, url))
                
                # Executa todas as extra√ß√µes em paralelo
                resultados_tasks = await asyncio.gather(*tasks)
                
                # Filtra apenas os resultados bem-sucedidos
                resultados_sucesso = [res for res in resultados_tasks if res]

                # Limita ao n√∫mero m√≠nimo de sucessos desejado
                resultados_sucesso = resultados_sucesso[:self.config['min_sucessos_por_termo']]

            print(f"üéØ Pesquisa para '{termo}' conclu√≠da com {len(resultados_sucesso)} extra√ß√µes bem-sucedidas.")
            return resultados_sucesso

        except Exception as e:
            print(f"‚ö†Ô∏è Falha cr√≠tica na busca do Google para '{termo}': {e}")
            return resultados_sucesso

    async def pesquisar_modelos_async(self, fundamentos: List[str]) -> Dict[str, Any]:
        tasks = [self._pesquisar_e_extrair_async(fundamento) for fundamento in fundamentos]
        resultados_brutos = await asyncio.gather(*tasks)
        
        todos_conteudos = [item for sublist in resultados_brutos for item in sublist]
        
        pesquisa_formatada = "## Modelos e Cl√°usulas de Refer√™ncia Encontrados:\n\n"
        for item in todos_conteudos:
            pesquisa_formatada += f"### Fonte: {item['url']}\n\n"
            pesquisa_formatada += f"```text\n{item['texto'][:2000]}...\n```\n\n---\n\n"
            
        return {"pesquisa_formatada": pesquisa_formatada, "conteudos_extraidos": todos_conteudos}

    def pesquisar_fundamentacao_completa(self, fundamentos: List[str], **kwargs) -> Dict[str, Any]:
        """Ponto de entrada s√≠ncrono que executa a l√≥gica ass√≠ncrona."""
        inicio_pesquisa = datetime.now()
        try:
            resultado = asyncio.run(self.pesquisar_modelos_async(fundamentos))
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico durante a pesquisa de contratos: {e}")
            return {"pesquisa_formatada": "A pesquisa de modelos de contrato falhou.", "conteudos_extraidos": []}
        
        tempo_total = (datetime.now() - inicio_pesquisa).total_seconds()
        
        print("\n--- RESUMO DA PESQUISA DE CONTRATOS ---")
        print(f"Fundamentos pesquisados: {fundamentos}")
        conteudos_encontrados = resultado.get("conteudos_extraidos", [])
        if conteudos_encontrados:
            print(f"‚úÖ {len(conteudos_encontrados)} modelos/conte√∫dos relevantes encontrados.")
            for i, item in enumerate(conteudos_encontrados, 1):
                print(f"  {i}. {item['url']} ({len(item['texto'])} chars)")
        else:
            print("‚ö†Ô∏è Nenhum modelo ou conte√∫do relevante foi extra√≠do com sucesso.")
        print(f"‚úÖ PESQUISA DE CONTRATOS CONCLU√çDA em {tempo_total:.1f} segundos\n")
        
        return resultado
