# agente_pesquisador_jurisprudencia.py - v2.1 (Com Pesquisa Aprofundada)

import asyncio
import aiohttp
import re
from datetime import datetime
from typing import Dict, Any, List
from googlesearch import search
from bs4 import BeautifulSoup

class AgentePesquisadorJurisprudencia:
    """
    Agente Especializado em Pesquisa de Jurisprud√™ncia.
    v2.1: Realiza uma pesquisa mais aprofundada, buscando garantir no m√≠nimo 10 resultados positivos.
    """
    def __init__(self):
        print("‚öñÔ∏è  Inicializando Agente de Pesquisa de JURISPRUD√äNCIA (v2.1)...")
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        # COMENT√ÅRIO: As configura√ß√µes foram ajustadas para uma pesquisa mais aprofundada.
        # A meta de sucessos foi aumentada para 10 e, para suportar isso,
        # o n√∫mero de resultados do Google tamb√©m foi aumentado.
        self.config = {
            'tamanho_minimo_conteudo': 500,
            'min_sucessos_por_termo': 10,
            'google_search_results': 20,
        }
        self.sites_prioritarios = ['stj.jus.br', 'stf.jus.br', 'tst.jus.br', 'conjur.com.br', 'migalhas.com.br']
        print("‚úÖ Sistema de pesquisa de JURISPRUD√äNCIA inicializado.")

    async def _extrair_conteudo_url_async(self, session, url: str) -> Dict[str, Any]:
        """Extrai conte√∫do de uma URL de forma ass√≠ncrona."""
        print(f"‚Üí Tentando extrair de: {url}")
        try:
            async with session.get(url, headers=self.headers, timeout=15, ssl=False) as response:
                if response.status == 200:
                    raw_html = await response.read()
                    html = raw_html.decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html, 'html.parser')
                    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                        tag.decompose()
                    
                    texto = soup.body.get_text(separator=' ', strip=True) if soup.body else ""
                    texto_limpo = re.sub(r'\s+', ' ', texto).strip()
                    
                    if len(texto_limpo) < self.config['tamanho_minimo_conteudo']:
                        print(f"‚ö†Ô∏è Descartado (curto): {url}")
                        return None

                    print(f"‚úî SUCESSO: Conte√∫do extra√≠do de {url} ({len(texto_limpo)} caracteres)")
                    return { "url": url, "texto": texto_limpo, "titulo": soup.title.string.strip() if soup.title else "N/A" }
                else:
                    print(f"‚ùå Falha (Status {response.status}): {url}")
                    return None
        except Exception as e:
            print(f"‚ùå Falha (Erro: {type(e).__name__}): {url}")
            return None

    async def _pesquisar_termo_async(self, termo: str) -> List[Dict[str, Any]]:
        """Busca um √∫nico termo e extrai o conte√∫do at√© atingir a meta."""
        print(f"\nüìö Buscando jurisprud√™ncia para o termo: '{termo}'...")
        site_query = " OR ".join([f"site:{site}" for site in self.sites_prioritarios])
        query = f'jurisprud√™ncia ementa ac√≥rd√£o sobre "{termo}" {site_query}'
        
        resultados_sucesso = []
        try:
            loop = asyncio.get_event_loop()
            urls_google = await loop.run_in_executor(None, lambda: list(search(query, num_results=self.config['google_search_results'], lang="pt")))
            
            async with aiohttp.ClientSession() as session:
                for url in urls_google:
                    if len(resultados_sucesso) >= self.config['min_sucessos_por_termo']:
                        break
                    resultado = await self._extrair_conteudo_url_async(session, url)
                    if resultado:
                        resultados_sucesso.append(resultado)
            
            print(f"üéØ Pesquisa para '{termo}' conclu√≠da com {len(resultados_sucesso)} extra√ß√µes bem-sucedidas.")
            return resultados_sucesso
        except Exception as e:
            print(f"‚ö†Ô∏è Falha cr√≠tica na busca do Google para '{termo}': {e}")
            return resultados_sucesso

    async def pesquisar_jurisprudencia_async(self, termos: List[str]) -> List[Dict[str, Any]]:
        """Cria e executa todas as tarefas de pesquisa em paralelo."""
        tasks = [self._pesquisar_termo_async(termo) for termo in termos]
        resultados_por_termo = await asyncio.gather(*tasks)
        
        todos_os_resultados = [item for sublist in resultados_por_termo for item in sublist]
        return todos_os_resultados

    def pesquisar(self, termos: List[str]) -> List[Dict[str, Any]]:
        """Ponto de entrada s√≠ncrono que executa a l√≥gica ass√≠ncrona."""
        inicio_pesquisa = datetime.now()
        try:
            resultado = asyncio.run(self.pesquisar_jurisprudencia_async(termos))
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico durante a pesquisa de jurisprud√™ncia: {e}")
            return []
        
        tempo_total = (datetime.now() - inicio_pesquisa).total_seconds()
        print(f"\n--- RESUMO DA PESQUISA DE JURISPRUD√äNCIA ---")
        print(f"‚úÖ Total de {len(resultado)} conte√∫dos relevantes encontrados.")
        print(f"‚úÖ PESQUISA CONCLU√çDA em {tempo_total:.1f} segundos\n")
        return resultado
