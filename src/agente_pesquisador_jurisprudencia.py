# agente_pesquisador_jurisprudencia.py - v4.5 (Com Estrat√©gia Anti-Bloqueio Aprimorada)

import asyncio
import aiohttp
import re
import openai
import os
import random
from datetime import datetime, timedelta
from typing import Dict, Any, List
from googlesearch import search
from bs4 import BeautifulSoup
from urllib.parse import urlparse

class AgentePesquisadorJurisprudencia:
    """
    Agente Especializado em Pesquisa de Jurisprud√™ncia.
    v4.5: Implementa uma estrat√©gia de extra√ß√£o "humanizada", acessando um link de cada
    dom√≠nio por vez e com pausas, para evitar bloqueios.
    """
    def __init__(self, api_key: str = None):
        print("‚öñÔ∏è  Inicializando Agente de Pesquisa de JURISPRUD√äNCIA (v4.5)...")
        
        if not api_key:
            api_key = os.getenv('DEEPSEEK_API_KEY')
        
        if not api_key:
            raise ValueError("A chave da API da DeepSeek √© necess√°ria para o filtro de relev√¢ncia e n√£o foi encontrada.")
        
        self.client = openai.OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
        
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
        ]
        
        self.headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        self.config = {
            'tamanho_minimo_conteudo': 300,
            'min_sucessos_por_termo': 10,
            'google_search_results': 20,
            'timeout_geral_pesquisa': 120,
        }
        self.sites_prioritarios = ['stj.jus.br', 'stf.jus.br', 'tst.jus.br', 'conjur.com.br', 'migalhas.com.br', 'ambito-juridico.com.br', 'ibdfam.org.br']
        print("‚úÖ Sistema de pesquisa de JURISPRUD√äNCIA inicializado.")

    async def _validar_relevancia_com_ia_async(self, texto: str, termo_pesquisa: str) -> bool:
        # ... (c√≥digo de valida√ß√£o com IA permanece o mesmo)
        try:
            prompt = f"""
            Analise o seguinte texto e determine se ele √© uma JURISPRUD√äNCIA (decis√£o judicial, ac√≥rd√£o, ementa) relevante para o termo de pesquisa "{termo_pesquisa}".
            Responda APENAS com "SIM" se for uma jurisprud√™ncia relevante, ou "N√ÉO" caso contr√°rio.

            TEXTO PARA AN√ÅLISE:
            ---
            {texto[:2000]}
            ---
            """
            response = await asyncio.to_thread(
                self.client.chat.completions.create,
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=10,
                temperature=0.0
            )
            resposta = response.choices[0].message.content.strip().upper()
            return "SIM" in resposta
        except Exception as e:
            print(f"‚ö†Ô∏è Erro na valida√ß√£o com IA: {e}")
            return False

    async def _extrair_e_validar_async(self, session, url: str, termo_pesquisa: str) -> Dict[str, Any]:
        # ... (c√≥digo de extra√ß√£o via Google Cache permanece o mesmo)
        cached_url = f"http://webcache.googleusercontent.com/search?q=cache:{url}"
        print(f"‚Üí Tentando extrair de (via cache): {url}")
        try:
            request_headers = self.headers.copy()
            request_headers['User-Agent'] = random.choice(self.user_agents)

            async with session.get(cached_url, headers=request_headers, timeout=20, ssl=False) as response:
                if response.status == 200:
                    raw_html = await response.read()
                    html = raw_html.decode('utf-8', errors='ignore')
                    soup = BeautifulSoup(html, 'html.parser')
                    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 'header', 'aside']):
                        tag.decompose()
                    
                    texto = soup.body.get_text(separator=' ', strip=True) if soup.body else ""
                    texto_limpo = re.sub(r'\s+', ' ', texto).strip()
                    
                    if len(texto_limpo) < self.config['tamanho_minimo_conteudo']:
                        print(f"‚ö†Ô∏è Descartado (curto): {url}")
                        return None

                    print(f"  -> Validando relev√¢ncia do conte√∫do com IA...")
                    if await self._validar_relevancia_com_ia_async(texto_limpo, termo_pesquisa):
                        print(f"‚úî SUCESSO (IA APROVOU): Conte√∫do extra√≠do de {url} ({len(texto_limpo)} caracteres)")
                        return { "url": url, "texto": texto_limpo, "titulo": soup.title.string.strip() if soup.title else "N/A" }
                    else:
                        print(f"‚ö†Ô∏è Descartado (IA Reprovou como irrelevante): {url}")
                        return None
                else:
                    print(f"‚ùå Falha (Status {response.status}): {url}")
                    return None
        except Exception as e:
            print(f"‚ùå Falha (Erro: {type(e).__name__}): {url}")
            return None

    async def _pesquisar_termo_async(self, termo: str) -> List[Dict[str, Any]]:
        """
        COMENT√ÅRIO: L√≥gica principal aprimorada. Agora ele organiza os links por dom√≠nio
        e os acessa de forma intercalada para simular um comportamento mais humano.
        """
        print(f"\nüìö Buscando jurisprud√™ncia para o termo: '{termo}' (modo estrat√©gico)...")
        
        resultados_sucesso = []
        tempo_limite = timedelta(seconds=self.config['timeout_geral_pesquisa'])
        inicio_pesquisa = datetime.now()
        
        try:
            loop = asyncio.get_event_loop()
            
            dominios_query = " OR ".join([f"site:{site}" for site in self.sites_prioritarios])
            query = f'"{termo}" jurisprud√™ncia ementa ac√≥rd√£o {dominios_query}'
            
            urls_encontradas = await loop.run_in_executor(None, lambda: list(search(query, num_results=self.config['google_search_results'], lang="pt")))
            
            # 1. Organiza as URLs por dom√≠nio
            urls_por_dominio = {}
            for url in urls_encontradas:
                if "/busca?" not in url:
                    dominio = urlparse(url).netloc
                    if dominio not in urls_por_dominio:
                        urls_por_dominio[dominio] = []
                    urls_por_dominio[dominio].append(url)

            # 2. Processa de forma intercalada
            async with aiohttp.ClientSession() as session:
                while datetime.now() - inicio_pesquisa < tempo_limite:
                    if len(resultados_sucesso) >= self.config['min_sucessos_por_termo']:
                        print(f"üéØ Meta de {self.config['min_sucessos_por_termo']} sucessos atingida.")
                        break
                    
                    # Pega um link de cada dom√≠nio para a pr√≥xima rodada
                    links_da_rodada = []
                    for dominio, urls in urls_por_dominio.items():
                        if urls:
                            links_da_rodada.append(urls.pop(0)) # Pega e remove o primeiro link da lista
                    
                    if not links_da_rodada:
                        print("  -> Todos os links encontrados foram processados.")
                        break

                    print(f"  -> Nova rodada de extra√ß√£o com {len(links_da_rodada)} links de dom√≠nios diferentes...")
                    tasks = [self._extrair_e_validar_async(session, url, termo) for url in links_da_rodada]
                    resultados_tasks = await asyncio.gather(*tasks)
                    
                    novos_sucessos = [res for res in resultados_tasks if res]
                    resultados_sucesso.extend(novos_sucessos)
                    
                    # Pausa para simular comportamento humano
                    await asyncio.sleep(random.uniform(2, 4))
            else:
                print(f"‚è∞ Tempo limite de {self.config['timeout_geral_pesquisa']}s atingido.")

            return resultados_sucesso[:self.config['min_sucessos_por_termo']]

        except Exception as e:
            print(f"‚ö†Ô∏è Falha cr√≠tica na busca: {e}")
            return resultados_sucesso

    async def pesquisar_jurisprudencia_async(self, termos: List[str]) -> List[Dict[str, Any]]:
        """Cria e executa todas as tarefas de pesquisa em paralelo."""
        tasks = [self._pesquisar_termo_async(termo) for termo in termos]
        resultados_por_termo = await asyncio.gather(*tasks)
        
        todos_os_resultados = [item for sublist in resultados_por_termo for item in sublist]
        return todos_os_resultados

    def pesquisar(self, termos: List[str]) -> List[Dict[str, Any]]:
        """Ponto de entrada s√≠ncrono que executa a l√≥gica ass√≠ncrona."""
        inicio_pesquisa = datetime.now()
        try:
            resultado = asyncio.run(self.pesquisar_jurisprudencia_async(termos))
        except Exception as e:
            print(f"‚ùå Erro cr√≠tico durante a pesquisa de jurisprud√™ncia: {e}")
            return []
        
        tempo_total = (datetime.now() - inicio_pesquisa).total_seconds()
        print(f"\n--- RESUMO DA PESQUISA DE JURISPRUD√äNCIA ---")
        print(f"‚úÖ Total de {len(resultado)} conte√∫dos relevantes encontrados.")
        print(f"‚úÖ PESQUISA CONCLU√çDA em {tempo_total:.1f} segundos\n")
        return resultado
